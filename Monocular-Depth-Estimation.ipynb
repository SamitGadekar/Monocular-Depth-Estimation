{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:01.497513Z",
     "start_time": "2025-08-09T05:51:00.730432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup the environment and import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg') # since PyCharm has some color grading effects\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path"
   ],
   "id": "c85f88768074c05",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:04.485583Z",
     "start_time": "2025-08-09T05:51:02.426183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import cv2"
   ],
   "id": "be16843bb596c249",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:13.487641Z",
     "start_time": "2025-08-09T05:51:13.476053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# https://figshare.com/articles/dataset/Indoor-Outdoor_dataset/4595323\n",
    "# Randomly sample images from the indoor outdoor dataset\n",
    "\n",
    "# Read in sample images\n",
    "data_root = Path(\"data\")\n",
    "data_folder_path = data_root / \"Indoor-Outdoor-JPGs\"\n",
    "indoor_folder_path = data_folder_path / \"indoor\"\n",
    "outdoor_folder_path = data_folder_path / \"outdoor\"\n",
    "num_samples = 3\n",
    "\n",
    "selected_indoor_images = random.sample(os.listdir(indoor_folder_path), num_samples)\n",
    "selected_outdoor_images = random.sample(os.listdir(outdoor_folder_path), num_samples)\n",
    "# elected_indoor_images = [\"1 (1).jpg\"]\n",
    "# selected_outdoor_images = [\"1 (1).jpg\"]\n",
    "\n",
    "indoor_images = []\n",
    "outdoor_images = []\n",
    "for i in range(num_samples):\n",
    "    # Flip from BGR to RGB since jpg, and add to list\n",
    "    indoor_image = cv2.imread(str(indoor_folder_path / selected_indoor_images[i]), cv2.IMREAD_UNCHANGED)\n",
    "    indoor_image = cv2.cvtColor(indoor_image, cv2.COLOR_BGR2RGB)\n",
    "    # indoor_image = 255 - indoor_image\n",
    "    outdoor_image = cv2.imread(str(outdoor_folder_path / selected_outdoor_images[i]), cv2.IMREAD_UNCHANGED)\n",
    "    outdoor_image = cv2.cvtColor(outdoor_image, cv2.COLOR_BGR2RGB)\n",
    "    # outdoor_image = 255 - outdoor_image\n",
    "\n",
    "    indoor_images.append(indoor_image)\n",
    "    outdoor_images.append(outdoor_image)\n",
    "\n",
    "    # Check that shapes are reasonable\n",
    "    print(indoor_image.shape, outdoor_image.shape)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 210, 3) (600, 407, 3)\n",
      "(469, 725, 3) (600, 407, 3)\n",
      "(182, 276, 3) (407, 600, 3)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:18.133602Z",
     "start_time": "2025-08-09T05:51:18.130736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting accelerator to either cuda (Nvidia), metal performance shaders (mps on Mac), or default cpu\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else (torch.device(\"mps\") if torch.backends.mps.is_built() else torch.device(\"cpu\"))\n",
    "print(device)"
   ],
   "id": "8dd885efcf97d3bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:22.013958Z",
     "start_time": "2025-08-09T05:51:20.607150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# https://github.com/LiheYoung/Depth-Anything\n",
    "# Import depth anything model small, base, and large\n",
    "processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-large-hf\").to(device)\n",
    "\n",
    "print(model.config)"
   ],
   "id": "3f809df17f8caf57",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepthAnythingConfig {\n",
      "  \"architectures\": [\n",
      "    \"DepthAnythingForDepthEstimation\"\n",
      "  ],\n",
      "  \"backbone\": null,\n",
      "  \"backbone_config\": {\n",
      "    \"apply_layernorm\": true,\n",
      "    \"architectures\": [\n",
      "      \"Dinov2Model\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"drop_path_rate\": 0.0,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 518,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_eps\": 1e-06,\n",
      "    \"layerscale_value\": 1.0,\n",
      "    \"mlp_ratio\": 4,\n",
      "    \"model_type\": \"dinov2\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"out_features\": [\n",
      "      \"stage21\",\n",
      "      \"stage22\",\n",
      "      \"stage23\",\n",
      "      \"stage24\"\n",
      "    ],\n",
      "    \"out_indices\": [\n",
      "      21,\n",
      "      22,\n",
      "      23,\n",
      "      24\n",
      "    ],\n",
      "    \"patch_size\": 14,\n",
      "    \"qkv_bias\": true,\n",
      "    \"reshape_hidden_states\": false,\n",
      "    \"stage_names\": [\n",
      "      \"stem\",\n",
      "      \"stage1\",\n",
      "      \"stage2\",\n",
      "      \"stage3\",\n",
      "      \"stage4\",\n",
      "      \"stage5\",\n",
      "      \"stage6\",\n",
      "      \"stage7\",\n",
      "      \"stage8\",\n",
      "      \"stage9\",\n",
      "      \"stage10\",\n",
      "      \"stage11\",\n",
      "      \"stage12\",\n",
      "      \"stage13\",\n",
      "      \"stage14\",\n",
      "      \"stage15\",\n",
      "      \"stage16\",\n",
      "      \"stage17\",\n",
      "      \"stage18\",\n",
      "      \"stage19\",\n",
      "      \"stage20\",\n",
      "      \"stage21\",\n",
      "      \"stage22\",\n",
      "      \"stage23\",\n",
      "      \"stage24\"\n",
      "    ],\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"use_mask_token\": true,\n",
      "    \"use_swiglu_ffn\": false\n",
      "  },\n",
      "  \"backbone_kwargs\": null,\n",
      "  \"depth_estimation_type\": \"relative\",\n",
      "  \"fusion_hidden_size\": 256,\n",
      "  \"head_hidden_size\": 32,\n",
      "  \"head_in_index\": -1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_depth\": 1,\n",
      "  \"model_type\": \"depth_anything\",\n",
      "  \"neck_hidden_sizes\": [\n",
      "    256,\n",
      "    512,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"output_attentions\": false,\n",
      "  \"patch_size\": 14,\n",
      "  \"reassemble_factors\": [\n",
      "    4,\n",
      "    2,\n",
      "    1,\n",
      "    0.5\n",
      "  ],\n",
      "  \"reassemble_hidden_size\": 1024,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"use_pretrained_backbone\": false,\n",
      "  \"use_timm_backbone\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:27.273966Z",
     "start_time": "2025-08-09T05:51:25.027851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pre-process the images\n",
    "indoor_samples = []\n",
    "outdoor_samples = []\n",
    "for i in range(num_samples):\n",
    "    indoor_input = processor(images=indoor_images[i], return_tensors=\"pt\").to(device)\n",
    "    outdoor_input = processor(images=outdoor_images[i], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Infer model\n",
    "    with torch.no_grad():\n",
    "        indoor_outputs = model(**indoor_input)\n",
    "        outdoor_outputs = model(**outdoor_input)\n",
    "        indoor_depth = indoor_outputs.predicted_depth\n",
    "        outdoor_depth = outdoor_outputs.predicted_depth\n",
    "\n",
    "    # Pytorch tensors to numpy arrays\n",
    "    indoor_depth = indoor_depth.squeeze().cpu().numpy()\n",
    "    outdoor_depth = outdoor_depth.squeeze().cpu().numpy()\n",
    "\n",
    "    indoor_samples.append([indoor_images[i], indoor_depth])\n",
    "    outdoor_samples.append([outdoor_images[i], outdoor_depth])"
   ],
   "id": "7cefe053aa768978",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:25:11.390247Z",
     "start_time": "2025-08-09T06:17:13.649424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualize the depth image and original images side by side\n",
    "fig, axs = plt.subplots(num_samples, 4, figsize=(12, 3*num_samples))\n",
    "if num_samples == 1:\n",
    "    axs = axs.reshape(1, 4)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    axs[i, 0].imshow(indoor_samples[i][0])\n",
    "    axs[i, 0].set_title(\"Indoor Original\")\n",
    "    axs[i, 0].axis('off')\n",
    "\n",
    "    axs[i, 1].imshow(indoor_samples[i][1])\n",
    "    axs[i, 1].set_title(\"Indoor Depth\")\n",
    "    axs[i, 1].axis('off')\n",
    "\n",
    "    axs[i, 2].imshow(outdoor_samples[i][0])\n",
    "    axs[i, 2].set_title(\"Outdoor Original\")\n",
    "    axs[i, 2].axis('off')\n",
    "\n",
    "    axs[i, 3].imshow(outdoor_samples[i][1])\n",
    "    axs[i, 3].set_title(\"Outdoor Depth\")\n",
    "    axs[i, 3].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9b9dfa9402211458",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:36.611271Z",
     "start_time": "2025-08-09T05:51:36.608483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_intrinsics(H, W, fov=55.0):\n",
    "    \"\"\"\n",
    "    Intrinsics for a pinhole camera model.\n",
    "    Assume fov of 55 degrees and central principle point.\n",
    "    \"\"\"\n",
    "    f = 0.5 * W / np.tan((np.pi / 2) * fov / 180.0)\n",
    "    cx = 0.5 * W\n",
    "    cy = 0.5 * H\n",
    "    # Assuming square pixels so can use f for both x and y\n",
    "    return np.array([[f, 0, cx],\n",
    "                     [0, f, cy],\n",
    "                     [0, 0, 1]])"
   ],
   "id": "b2e2024d7a2e9e7f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:51:38.942809Z",
     "start_time": "2025-08-09T05:51:38.939199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pixel_to_point(depth_image, camera_intrinsics=None):\n",
    "    \"\"\"\n",
    "    Converts depth image to 3D points.\n",
    "    Assumes fov of 55 degrees and central principle point.\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = depth_image.shape\n",
    "    if camera_intrinsics is None:\n",
    "        camera_intrinsics = get_intrinsics(height, width)\n",
    "\n",
    "    # Create u, v mesh grid and precompute projection triangle ratios\n",
    "    fx, fy = camera_intrinsics[0,0], camera_intrinsics[1,1]\n",
    "    cx, cy = camera_intrinsics[0,2], camera_intrinsics[1,2]\n",
    "\n",
    "    # Blank mesh grid of correct size\n",
    "    x = np.linspace(0, width - 1, width)\n",
    "    y = np.linspace(0, height - 1, height)\n",
    "    u, v = np.meshgrid(x, y)\n",
    "\n",
    "    x_over_z = (u - cx) / fx\n",
    "    y_over_z = (v - cy) / fy\n",
    "\n",
    "    # 3D Pythagorean theorem rearranged to solve for z\n",
    "    z = depth_image / np.sqrt(1.0 + x_over_z**2 + y_over_z**2)\n",
    "    x = x_over_z * z\n",
    "    y = y_over_z * z\n",
    "\n",
    "    return x, y, z"
   ],
   "id": "a8f06e666d986605",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:14:48.974280Z",
     "start_time": "2025-08-09T06:14:48.968492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_point_cloud(depth_image, color_image, camera_intrinsics=None, scale_ratio=100.0):\n",
    "    height, width = depth_image.shape\n",
    "    if camera_intrinsics is None:\n",
    "        camera_intrinsics = get_intrinsics(height, width)\n",
    "\n",
    "    # Rescaling color image to the size of the depth image\n",
    "    # since DepthAnything was trained on only 518x518 models,\n",
    "    # it automatically resizes its inputs\n",
    "    color_image = cv2.resize(color_image, (width, height))\n",
    "\n",
    "    # Making sure depth image does not contain any zeroes\n",
    "    depth_image = np.maximum(depth_image, 1e-5)\n",
    "\n",
    "    depth_image = scale_ratio / depth_image\n",
    "    x, y, z = pixel_to_point(depth_image, camera_intrinsics)\n",
    "    point_image = np.stack((x, y, z), axis=-1)\n",
    "    # xyzrgb_image = np.concatenate([point_image, color_image], axis=-1)\n",
    "    # xyzrgb not needed as o3d has that capability\n",
    "\n",
    "    cloud = o3d.geometry.PointCloud()\n",
    "    cloud.points = o3d.utility.Vector3dVector(point_image.reshape(-1, 3))\n",
    "    cloud.colors = o3d.utility.Vector3dVector(color_image.reshape(-1, 3) / 255.0)\n",
    "\n",
    "    # Masking for outdoor skies\n",
    "    mask = point_image[:,:,2] < 1e3\n",
    "    cloud.points = o3d.utility.Vector3dVector(point_image[mask].reshape(-1, 3))\n",
    "    cloud.colors = o3d.utility.Vector3dVector(color_image[mask].reshape(-1, 3) / 255.0)\n",
    "\n",
    "    return cloud"
   ],
   "id": "a346e0a51ab14861",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:15:20.781306Z",
     "start_time": "2025-08-09T06:15:20.556786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save point-cloud files\n",
    "output_path = data_root / \"point_clouds\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    cloud = create_point_cloud(depth_image=indoor_samples[i][1], color_image=indoor_samples[i][0])\n",
    "    o3d.io.write_point_cloud(output_path / f\"indoor_point_cloud_{i}.ply\", cloud)\n",
    "\n",
    "    cloud = create_point_cloud(depth_image=outdoor_samples[i][1], color_image=outdoor_samples[i][0])\n",
    "    o3d.io.write_point_cloud(output_path / f\"outdoor_point_cloud_{i}.ply\", cloud)"
   ],
   "id": "3821fa6cf786412c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:28:00.734149Z",
     "start_time": "2025-08-09T06:27:30.383284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualize your point-cloud in an interactive window\n",
    "version, number = \"indoor\", 2\n",
    "cloud_path = output_path / f\"{version}_point_cloud_{number}.ply\"\n",
    "pcd = o3d.io.read_point_cloud(cloud_path)\n",
    "o3d.visualization.draw_geometries([pcd])\n"
   ],
   "id": "9dee27e16bd2c383",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:31:10.133411Z",
     "start_time": "2025-08-09T07:29:50.431783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualize your point-cloud in an interactive window (fov slider)\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "default_fov = 55\n",
    "default_scale = 1.0\n",
    "default_offset = 0.0\n",
    "class FOVVisualizerApp:\n",
    "    def __init__(self, master, depth_image, color_image):\n",
    "        self.master = master\n",
    "        self.depth_image = depth_image\n",
    "        self.color_image = color_image\n",
    "\n",
    "        self.master.title(\"FOV Camera Intrinsics Visualizer\")\n",
    "\n",
    "        # FOV slider and label\n",
    "        label = ttk.Label(master, text=\"Field of View (degrees):\")\n",
    "        label.pack(pady=5)\n",
    "\n",
    "        self.fov_slider = ttk.Scale(master, from_=10, to=120, orient='horizontal',\n",
    "                                    command=self.update_fov_or_scale)\n",
    "        self.fov_slider.set(default_fov)\n",
    "        self.fov_slider.pack(fill='x', padx=10, pady=5)\n",
    "\n",
    "        self.fov_value_label = ttk.Label(master, text=f\"FOV: {default_fov}°\")\n",
    "        self.fov_value_label.pack(pady=(0, 10))\n",
    "\n",
    "        # Depth scale slider and label\n",
    "        label_scale = ttk.Label(master, text=\"Depth Scale (larger = bigger):\")\n",
    "        label_scale.pack(pady=5)\n",
    "\n",
    "        self.scale_slider = ttk.Scale(master, from_=1, to=10, orient='horizontal',\n",
    "                                     command=self.update_fov_or_scale)\n",
    "        self.scale_slider.set(default_scale)\n",
    "        self.scale_slider.pack(fill='x', padx=10, pady=5)\n",
    "\n",
    "        self.scale_value_label = ttk.Label(master, text=f\"Depth Scale: {default_scale}\")\n",
    "        self.scale_value_label.pack(pady=(0, 10))\n",
    "\n",
    "        # Offset slider and label\n",
    "        label_offset = ttk.Label(master, text=\"Offset (larger = closer):\")\n",
    "        label_offset.pack(pady=5)\n",
    "\n",
    "        self.offset_slider = ttk.Scale(master, from_=-5, to=20, orient='horizontal',\n",
    "                                     command=self.update_fov_or_scale)\n",
    "        self.offset_slider.set(default_offset)\n",
    "        self.offset_slider.pack(fill='x', padx=10, pady=5)\n",
    "\n",
    "        self.offset_value_label = ttk.Label(master, text=f\"Offset: {default_offset}\")\n",
    "        self.offset_value_label.pack(pady=(0, 10))\n",
    "\n",
    "        # Open3D visualizer init\n",
    "        self.vis = o3d.visualization.Visualizer()\n",
    "        self.vis.create_window(window_name=\"Point Cloud\", width=800, height=600, visible=True)\n",
    "        self.pcd = None\n",
    "\n",
    "        # Initial display\n",
    "        self.update_point_cloud(default_fov, default_scale, default_offset)\n",
    "\n",
    "        # Start periodic visualizer update\n",
    "        self.update_visualizer()\n",
    "\n",
    "        self.master.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def update_point_cloud(self, fov, scale_ratio, offset):\n",
    "        fov = float(fov)\n",
    "        H, W = self.depth_image.shape\n",
    "        intrinsics = get_intrinsics(H, W, fov)\n",
    "\n",
    "        temp_depth = self.depth_image.copy() ** (1.0 / scale_ratio) + offset\n",
    "        cloud = create_point_cloud(temp_depth, self.color_image, intrinsics)\n",
    "\n",
    "        if self.pcd is None:\n",
    "            self.pcd = cloud\n",
    "            self.vis.add_geometry(self.pcd)\n",
    "        else:\n",
    "            self.pcd.points = cloud.points\n",
    "            self.pcd.colors = cloud.colors\n",
    "            self.vis.update_geometry(self.pcd)\n",
    "\n",
    "    def update_fov_or_scale(self, val):\n",
    "        # Get current slider values\n",
    "        current_fov = self.fov_slider.get()\n",
    "        current_scale = self.scale_slider.get()\n",
    "        current_offset = self.offset_slider.get()\n",
    "\n",
    "        # Update labels\n",
    "        self.fov_value_label.config(text=f\"FOV: {current_fov:.1f}°\")\n",
    "        self.scale_value_label.config(text=f\"Depth Scale: {int(current_scale)}\")\n",
    "        self.offset_value_label.config(text=f\"Offset: {int(current_offset)}\")\n",
    "\n",
    "        # Update point cloud with current parameters\n",
    "        self.update_point_cloud(current_fov, current_scale, current_offset)\n",
    "\n",
    "    def update_visualizer(self):\n",
    "        self.vis.poll_events()\n",
    "        self.vis.update_renderer()\n",
    "        self.master.after(30, self.update_visualizer)\n",
    "\n",
    "    def on_closing(self):\n",
    "        self.vis.destroy_window()\n",
    "        self.master.destroy()\n",
    "\n",
    "\n",
    "def run_visualizer():\n",
    "    dataset, index = outdoor_samples, 2\n",
    "\n",
    "    depth_image = dataset[index][1]\n",
    "    color_image = dataset[index][0]\n",
    "\n",
    "    root = tk.Tk()\n",
    "    app = FOVVisualizerApp(root, depth_image, color_image)\n",
    "    root.mainloop()\n",
    "\n",
    "run_visualizer()"
   ],
   "id": "1bef2440fb442c58",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sammygadekar/miniconda3/lib/python3.13/tkinter/__init__.py\", line 2068, in __call__\n",
      "    return self.func(*args)\n",
      "           ~~~~~~~~~^^^^^^^\n",
      "  File \"/var/folders/27/hd37c_md0nj25ymwhp4tsz_m0000gn/T/ipykernel_28217/700417097.py\", line 84, in update_fov_or_scale\n",
      "    current_scale = self.scale_slider.get()\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'FOVVisualizerApp' object has no attribute 'scale_slider'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sammygadekar/miniconda3/lib/python3.13/tkinter/__init__.py\", line 2068, in __call__\n",
      "    return self.func(*args)\n",
      "           ~~~~~~~~~^^^^^^^\n",
      "  File \"/var/folders/27/hd37c_md0nj25ymwhp4tsz_m0000gn/T/ipykernel_28217/700417097.py\", line 85, in update_fov_or_scale\n",
      "    current_offset = self.offset_slider.get()\n",
      "                     ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'FOVVisualizerApp' object has no attribute 'offset_slider'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sammygadekar/miniconda3/lib/python3.13/tkinter/__init__.py\", line 2068, in __call__\n",
      "    return self.func(*args)\n",
      "           ~~~~~~~~~^^^^^^^\n",
      "  File \"/var/folders/27/hd37c_md0nj25ymwhp4tsz_m0000gn/T/ipykernel_28217/700417097.py\", line 90, in update_fov_or_scale\n",
      "    self.offset_value_label.config(text=f\"Offset: {int(current_offset)}\")\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'FOVVisualizerApp' object has no attribute 'offset_value_label'. Did you mean: 'fov_value_label'?\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca2e5e23c00156e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
